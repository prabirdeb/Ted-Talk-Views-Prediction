{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prabirdeb/Ted-Talk-Views-Prediction/blob/main/Deployment_Code_Ted_Talk_Views_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKKZnwRgt1V9"
      },
      "source": [
        "**Ted Talk Views Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ag_J2iydtniR",
        "outputId": "24112fa6-01e7-454e-92bf-903222ac9199"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import math\n",
        "import ast\n",
        "from datetime import datetime\n",
        "from datetime import date\n",
        "import re\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import string\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # For test\n",
        "# ted_talk_df=pd.read_csv('/content/drive/MyDrive/Almabetter Assignments/Capstone projects/Ted Talk Views Prediction-Prabir Debnath/data_ted_talks_half.csv')"
      ],
      "metadata": {
        "id": "qGBjQYXobWLs"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jD46RoK4uQnq"
      },
      "outputs": [],
      "source": [
        "# Reading the data as pandas dataframe\n",
        "ted_talk_df=pd.read_csv('data_ted_talks_half.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_prep():\n",
        "  global ted_talk_df\n",
        "\n",
        "  # Finding out the relavant features from the deeper understanding of the data\n",
        "  relavant_features=['occupations', 'views', 'published_date',\n",
        "                   'native_lang', 'duration', 'topics','speaker_1','title']\n",
        "\n",
        "  # Creating new df with relavant features\n",
        "  ted_talk_df_clean=ted_talk_df[relavant_features]\n",
        "\n",
        "  # Imputaion of null values of occupation with emty dict string\n",
        "  ted_talk_df_clean['occupations']=ted_talk_df_clean['occupations'].fillna(\"{0:[]}\")\n",
        "\n",
        "  # There are python literals as string in the categorical columns which need to be treated\n",
        "  for col in ['occupations', 'topics']:\n",
        "    ted_talk_df_clean[col]=[ast.literal_eval(i) for i in ted_talk_df_clean[col]]\n",
        "\n",
        "  # extracting the list from the dict of occupations\n",
        "  ted_talk_df_clean['occupations']=[i.get(0) for i in ted_talk_df_clean['occupations']]\n",
        "\n",
        "  # Datetime is appearing as string and we are converting to datetime\n",
        "  ted_talk_df_clean['published_date']=ted_talk_df_clean['published_date'].apply(lambda x : datetime.strptime(x,'%Y-%m-%d'))\n",
        "\n",
        "  # Feature engineering on published date to to extract years run\n",
        "  ted_talk_df_clean['published_year']=ted_talk_df_clean['published_date'].apply(lambda x : x.year)\n",
        "  ted_talk_df_clean['base_year']=2021\n",
        "  ted_talk_df_clean['years_run']=(ted_talk_df_clean['base_year']-ted_talk_df_clean['published_year'])\n",
        "\n",
        "  ted_talk_df_clean.drop(['published_date','base_year','published_year'], axis = 1, inplace=True)\n",
        "\n",
        "  # There are very few experiences (only 1-27) for most of the 'native_lang' as compared to 'en' with count 3975. \n",
        "  # So, we are removing the exceptions in 'native_lang' category and creating a conditional df only for 'en'\n",
        "  ted_talk_df_clean=ted_talk_df_clean.loc[(ted_talk_df_clean['native_lang']=='en')].reset_index()\n",
        "  ted_talk_df_clean.drop('index', axis = 1, inplace=True)\n",
        "\n",
        "  # Thus we are dropping the 'native_lang' column\n",
        "  ted_talk_df_clean.drop('native_lang', axis = 1, inplace=True)\n",
        "\n",
        "  # There are mixture of words in topics and occupations column.\n",
        "  # Lets find out the main topics\n",
        "  main_topics=[]\n",
        "  for k in range(len(ted_talk_df_clean.topics)):\n",
        "    common_terms=list(set([i[:3] for i in ted_talk_df_clean.occupations[k]]).intersection(set([i[:3] for i in ted_talk_df_clean.topics[k]])))\n",
        "    \n",
        "    if len(common_terms)!=0:\n",
        "      for i in range(len(common_terms)):\n",
        "        pattern = re.compile(\"%s\" % common_terms[i])\n",
        "        topics=[x for x in ted_talk_df_clean.topics[k] if pattern.match(x)][0]\n",
        "    else:\n",
        "      topics='unknown'\n",
        "\n",
        "    main_topics.append(topics)\n",
        "\n",
        "  ted_talk_df_clean['main_topics']=main_topics\n",
        "\n",
        "  ted_talk_df_clean.drop(['occupations'], axis = 1, inplace=True)\n",
        "\n",
        "  topics_df=pd.DataFrame(ted_talk_df_clean.groupby('main_topics')['views'].mean().sort_values(ascending=False))\n",
        "  \n",
        "  # There is a great portion of 'unkown' topics which need to be treated\n",
        "  # Now, we can divide topics into three categories: Highly Favourite:2, Medium Favourite:1, Least Favourite:0\n",
        "  topics_df.rename(columns={'views':'views_mean'},inplace=True)\n",
        "  least_favourite=set(topics_df[(topics_df.views_mean<0.2*10**7)].index)-{'unknown'}\n",
        "  medium_favourite=set(topics_df[(topics_df.views_mean>=0.2*10**7) & (topics_df.views_mean< 0.5*10**7)].index)-{'unknown'}\n",
        "  highly_favourite=set(topics_df[(topics_df.views_mean>=0.5*10**7)].index)-{'unknown'}\n",
        "\n",
        "  topics_cat=[]\n",
        "  for k in ted_talk_df_clean.topics:\n",
        "    topics_least_favourite_match=len(list(set(k).intersection(least_favourite)))\n",
        "    topics_medium_favourite_match=len(list(set(k).intersection(medium_favourite)))\n",
        "    topics_highly_favourite_match=len(list(set(k).intersection(highly_favourite)))\n",
        "\n",
        "    if (topics_least_favourite_match>topics_medium_favourite_match) & (topics_least_favourite_match>topics_highly_favourite_match):\n",
        "      topics_cat.append(0)\n",
        "    elif (topics_medium_favourite_match>topics_least_favourite_match) & (topics_medium_favourite_match>topics_highly_favourite_match):\n",
        "      topics_cat.append(1)\n",
        "    else:\n",
        "      topics_cat.append(2)\n",
        "\n",
        "  ted_talk_df_clean['topics_cat']=topics_cat\n",
        "\n",
        "  ted_talk_df_clean.drop(['topics', 'main_topics'], axis = 1, inplace=True)\n",
        "\n",
        "  speaker_df=pd.DataFrame(ted_talk_df_clean.groupby('speaker_1')['views'].mean().sort_values(ascending=False))\n",
        "  \n",
        "  # Thus we can divide speakers into three categories: Highly Famous:2, Medium Famous:1, Least Famous:0\n",
        "  speaker_df.rename(columns={'views':'views_mean'},inplace=True)\n",
        "  ted_talk_df_clean = ted_talk_df_clean.merge(speaker_df,on = 'speaker_1',how = 'left')\n",
        "  ted_talk_df_clean['speaker_cat'] = ted_talk_df_clean['views_mean'].apply(lambda x : 0 if x < 0.4*10**7 else (1 if 0.4*10**7 <= x < 0.8*10**7 else 2))\n",
        "\n",
        "  ted_talk_df_clean.drop(['speaker_1','views_mean'],axis=1,inplace=True)\n",
        "\n",
        "  # Lets understand the sentiment of the title and encode \n",
        "\n",
        "  ted_talk_df_clean['title'] = ted_talk_df_clean['title'].apply(text_process)\n",
        "\n",
        "  # Extracting the highly attractive words from title\n",
        "  highly=''\n",
        "  for k in ted_talk_df_clean[(ted_talk_df_clean['views']>0.8*10**7)]['title'].values:\n",
        "    highly=highly+' '+k\n",
        "  highly_attractive=set(highly.split())\n",
        "\n",
        "  # Extracting the medium attractive words from title\n",
        "  medium=''\n",
        "  for k in ted_talk_df_clean[(ted_talk_df_clean['views']>=0.4*10**7) & (ted_talk_df_clean['views']<=0.8*10**7)]['title'].values:\n",
        "    medium=medium+' '+k\n",
        "  medium_attractive=set(medium.split())\n",
        "\n",
        "  # Extracting the least attractive words from title\n",
        "  least=''\n",
        "  for k in ted_talk_df_clean[(ted_talk_df_clean['views']<0.4*10**7)]['title'].values:\n",
        "    least=least+' '+k\n",
        "  least_attractive=set(least.split())\n",
        "\n",
        "  highly_attractive_words=highly_attractive-highly_attractive.intersection(medium_attractive)-highly_attractive.intersection(least_attractive)\n",
        "  medium_attractive_words=medium_attractive-medium_attractive.intersection(highly_attractive)-least_attractive.intersection(least_attractive)\n",
        "  least_attractive_words=least_attractive-least_attractive.intersection(medium_attractive)-least_attractive.intersection(highly_attractive)\n",
        "\n",
        "  # Title encoding\n",
        "  title_cat=[]\n",
        "  for k in ted_talk_df_clean.title:\n",
        "    least_attractive_words_match=len(list(set(k.split()).intersection(least_attractive_words)))\n",
        "    medium_attractive_words_match=len(list(set(k.split()).intersection(medium_attractive_words)))\n",
        "    highly_attractive_words_match=len(list(set(k.split()).intersection(highly_attractive_words)))\n",
        "\n",
        "    if (least_attractive_words_match>medium_attractive_words_match) & (least_attractive_words_match>highly_attractive_words_match):\n",
        "      title_cat.append(0)\n",
        "    elif (medium_attractive_words_match>least_attractive_words_match) & (medium_attractive_words_match>highly_attractive_words_match):\n",
        "      title_cat.append(1)\n",
        "    else:\n",
        "      title_cat.append(2)\n",
        "\n",
        "  ted_talk_df_clean['title_cat']=title_cat\n",
        "\n",
        "  ted_talk_df_clean.drop(['title'],axis=1,inplace=True)\n",
        "\n",
        "  # Arranging dependent feature in the last column\n",
        "  dependent=ted_talk_df_clean.views.values\n",
        "  ted_talk_df_clean.drop(['views'],axis=1,inplace=True)\n",
        "  ted_talk_df_clean['views']=dependent    \n",
        "\n",
        "  # There are only few experiences with more than 1*10^7 views. Thus we can remove these experiences\n",
        "  ted_talk_df_clean=ted_talk_df_clean[ted_talk_df_clean['views'] < 1*10**7]\n",
        "\n",
        "  # Creating dependent(output) and independent(input) variable\n",
        "  dependent_variable='views'\n",
        "  independent_variables=list(set(ted_talk_df_clean.describe().columns)-{dependent_variable})\n",
        "\n",
        "  ted_talk_df_clean[list(set(independent_variables)-{'duration'})]\n",
        "\n",
        "  # Creating normalized input and output dataset\n",
        "  X = np.log10(ted_talk_df_clean[['duration']])\n",
        "  X[list(set(independent_variables)-{'duration'})]=ted_talk_df_clean[list(set(independent_variables)-{'duration'})]\n",
        "\n",
        "  y = np.log10(ted_talk_df_clean[dependent_variable])\n",
        "\n",
        "  # Imputation of infinite values with zero\n",
        "  for col in X.columns:\n",
        "    X[col].replace([np.inf, -np.inf], 0, inplace=True)\n",
        "\n",
        "  y.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "\n",
        "  # Splitting of the data into Train and Test\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "\n",
        "  # Standardization of Input Data\n",
        "  scaler = StandardScaler()\n",
        "  X_train = scaler.fit_transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "\n",
        "  # Final model training\n",
        "  model_svr_final=SVR(C= 6, gamma= 0.1)\n",
        "  model_svr_final.fit(X_train, y_train)\n",
        "\n",
        "  return X_train, scaler, model_svr_final \n",
        "  \n"
      ],
      "metadata": {
        "id": "FKgPsqSn4gFH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "n6ZtEKdj2LUg"
      },
      "outputs": [],
      "source": [
        "# We can divide title into three categories: Highly Attractive:2, Medium Attractive:1, Least Attractive:0\n",
        "def text_process(text):\n",
        "    nopunc =[char for char in text if char not in string.punctuation]\n",
        "    nopunc=''.join(nopunc)\n",
        "    return ' '.join([word for word in nopunc.split() if word.lower() not in stopwords.words('english')])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing the function for predicting year of experience\n",
        "def final_svr(duration, topics_cat, years_run, speaker_cat, title_cat):\n",
        "  '''\n",
        "  This function is predicting views for ted talk videos\n",
        "\n",
        "  INPUT: \n",
        "  duration=int in seconds \n",
        "  topics_cat=Highly Favourite:2, Medium Favourite:1, Least Favourite:0 \n",
        "  years_run=int in years \n",
        "  speaker_cat=Highly Famous:2, Medium Famous:1, Least Famous:0\n",
        "  title_cat=Highly Attractive:2, Medium Attractive:1, Least Attractive:0\n",
        "  OUTPUT: y_test_preds_cat: predicted views for ted talk videos\n",
        "  \n",
        "  '''\n",
        "  try:\n",
        "\n",
        "    X_train, scaler, model_svr_final=data_prep()\n",
        "    \n",
        "    # Creating numpy array from the input\n",
        "    X_test=np.array([[duration, topics_cat, years_run, speaker_cat, title_cat]])\n",
        "    # log transformation on duration\n",
        "    X_test=np.array([[duration, topics_cat, years_run, speaker_cat, title_cat]])\n",
        "    a=np.log10(X_test[:,:1])\n",
        "    b=X_test[:,1:]\n",
        "    X_test=np.concatenate((a,b),axis=1)\n",
        "    # scaling of the input\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "      \n",
        "    # Checking model performance for test set\n",
        "    y_test_preds_cat = 10**model_svr_final.predict(X_test)\n",
        "\n",
        "  except:\n",
        "    print(\"Sorry ! Please check your input!\")\n",
        "  \n",
        "  return np.around(y_test_preds_cat, 2)[0]"
      ],
      "metadata": {
        "id": "pSw4B6U_CGWX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Streamlit Project\n",
        "import streamlit as st # All the text cell will be displayed after this import statement\n",
        "\n",
        "st.title(\"Ted Talk Videos Views Prediction\")\n",
        "st.header(\"Note for Topic Cat, Speaker Cat and Title Cat: Highly Favourite:2, Medium Favourite:1, Least Favourite:0 \")\n",
        "\n",
        "d = st.number_input(\"Duration (seconds)\")\n",
        "duration=d.title() # .title() is used to get the input\n",
        "\n",
        "to= st.number_input(\"Topic Cat (0, 1 or 2)\")\n",
        "topics_cat=to.title() # .title() is used to get the input\n",
        "\n",
        "y = st.number_input(\"Years Run\")\n",
        "years_run=y.title() # .title() is used to get the input\n",
        "\n",
        "s = st.number_input(\"Speaker Cat (0, 1 or 2)\")\n",
        "speaker_cat=s.title() # .title() is used to get the input\n",
        "\n",
        "t = st.number_input(\"Title Cat (0, 1 or 2)\")\n",
        "title_cat=t.title() # .title() is used to get the input\n",
        "\n",
        "ans = final_svr(duration, topics_cat, years_run, speaker_cat, title_cat)\n",
        "\n",
        "if(st.button('PREDICT')):   # display the ans when the submit button is clicked\n",
        "  st.success(ans)"
      ],
      "metadata": {
        "id": "TPr83t5vCZCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gO9IKYxgXFZg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Deployment Code Ted Talk Views Prediction.ipynb",
      "provenance": [],
      "mount_file_id": "13IaV7JVbAZzvig40ZxqGYBy1V1eS4IfF",
      "authorship_tag": "ABX9TyPA9PlCPi+F9+xYqr2bwzN1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}